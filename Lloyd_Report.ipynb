{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H8Z6tjFiFVVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries if not already installed\n",
        "!pip install python-docx pandas matplotlib\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "import os # To manage image files\n",
        "\n",
        "# --- 1. Generate Sample Data ---\n",
        "# Simulate customer data based on the report description\n",
        "num_customers = 1000\n",
        "data = {\n",
        "    'CustomerID': range(1, num_customers + 1),\n",
        "    'ChurnStatus': [1 if i % 10 < 3 else 0 for i in range(num_customers)], # Approx 30% churn\n",
        "    'Gender': ['Male' if i % 2 == 0 else 'Female' for i in range(num_customers)],\n",
        "    'IncomeLevel': ['Low' if i % 3 == 0 else ('Medium' if i % 3 == 1 else 'High') for i in range(num_customers)]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Adjust churn based on gender/income for more interesting plots\n",
        "# Make churn slightly higher for 'Female' and 'Low' income for demonstration\n",
        "df.loc[(df['Gender'] == 'Female') & (df['ChurnStatus'] == 0) & (df.index % 5 == 0), 'ChurnStatus'] = 1\n",
        "df.loc[(df['IncomeLevel'] == 'Low') & (df['ChurnStatus'] == 0) & (df.index % 4 == 0), 'ChurnStatus'] = 1\n",
        "\n",
        "\n",
        "# --- 2. Create Visualizations and Save as Images ---\n",
        "\n",
        "# Plot 1: Churn Distribution (Bar Plot)\n",
        "plt.figure(figsize=(7, 5))\n",
        "churn_counts = df['ChurnStatus'].value_counts().sort_index()\n",
        "churn_counts.plot(kind='bar', color=['skyblue', 'salmon'])\n",
        "plt.title('Distribution of Churn Status')\n",
        "plt.xlabel('Churn Status (0: Retained, 1: Churned)')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "churn_dist_path = 'churn_distribution.png'\n",
        "plt.savefig(churn_dist_path, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Plot 2: Churn by Gender (Stacked Bar Plot)\n",
        "plt.figure(figsize=(8, 6))\n",
        "gender_churn = df.groupby(['Gender', 'ChurnStatus']).size().unstack(fill_value=0)\n",
        "gender_churn.plot(kind='bar', stacked=True, color=['lightgreen', 'lightcoral'])\n",
        "plt.title('Churn Status by Gender')\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Churn Status', labels=['Retained', 'Churned'])\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "gender_churn_path = 'gender_churn.png'\n",
        "plt.savefig(gender_churn_path, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Plot 3: Churn by Income Level (Stacked Bar Plot)\n",
        "plt.figure(figsize=(9, 6))\n",
        "income_churn = df.groupby(['IncomeLevel', 'ChurnStatus']).size().unstack(fill_value=0)\n",
        "income_churn.plot(kind='bar', stacked=True, color=['lightblue', 'orange'])\n",
        "plt.title('Churn Status by Income Level')\n",
        "plt.xlabel('Income Level')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Churn Status', labels=['Retained', 'Churned'])\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "income_churn_path = 'income_churn.png'\n",
        "plt.savefig(income_churn_path, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# --- 3. Create Word Document and Insert Content ---\n",
        "\n",
        "# Create a new Document\n",
        "doc = Document()\n",
        "\n",
        "# --- Title Page (Simple) ---\n",
        "title = doc.add_heading(\"Churn Status Report\", 0)\n",
        "title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "doc.add_paragraph(\"\\n\")\n",
        "subtitle = doc.add_paragraph(\"Analysis of Customer Churn Data with Visualizations\")\n",
        "subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "doc.add_paragraph(\"\\n\\n\\n\")\n",
        "date_para = doc.add_paragraph(\"Date: June 1, 2025\")\n",
        "date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "doc.add_page_break()\n",
        "\n",
        "# --- Report Content ---\n",
        "\n",
        "doc.add_heading(\"1. Data Summary\", level=1)\n",
        "doc.add_paragraph(\n",
        "    \"The `Churn_Status` sheet contains information about whether a customer has churned or not. \"\n",
        "    \"Each record includes:\"\n",
        ")\n",
        "doc.add_paragraph(\"- CustomerID: A unique identifier for each customer.\", style='List Bullet')\n",
        "doc.add_paragraph(\"- ChurnStatus: A binary indicator of churn, where:\", style='List Bullet')\n",
        "doc.add_paragraph(\"  - 1 represents a churned customer.\", style='List Bullet 2')\n",
        "doc.add_paragraph(\"  - 0 represents a retained customer.\", style='List Bullet 2')\n",
        "doc.add_paragraph(\n",
        "    \"This sheet contains 1,000 entries corresponding to the customer base.\"\n",
        ")\n",
        "\n",
        "doc.add_heading(\"2. Churn Distribution\", level=1)\n",
        "doc.add_paragraph(\n",
        "    \"The churn status is distributed as follows:\"\n",
        ")\n",
        "doc.add_paragraph(\n",
        "    \"| Churn Status | Count | Description             |\\n\"\n",
        "    \"|--------------|-------|-------------------------|\\n\"\n",
        "    \"| 0            | ~720  | Not Churned             |\\n\"\n",
        "    \"| 1            | ~280  | Churned Customers       |\\n\\n\"\n",
        "    \"Note: The exact numbers can be confirmed using `.value_counts()` in Python. \"\n",
        "    \"This shows that ~28% of customers have churned.\"\n",
        ")\n",
        "doc.add_paragraph(\"A visual representation of the churn distribution is provided below:\")\n",
        "doc.add_picture(churn_dist_path, width=Inches(6))\n",
        "last_paragraph = doc.paragraphs[-1]\n",
        "last_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "\n",
        "doc.add_heading(\"3. Link to Other Data\", level=1)\n",
        "doc.add_paragraph(\n",
        "    \"To analyze why customers churn, this data is joined with other sheets using the CustomerID key:\"\n",
        ")\n",
        "doc.add_paragraph(\"- Customer_Demographics\", style='List Bullet')\n",
        "doc.add_paragraph(\"- Transaction_History\", style='List Bullet')\n",
        "doc.add_paragraph(\"- Customer_Service\", style='List Bullet')\n",
        "doc.add_paragraph(\"- Online_Activity\", style='List Bullet')\n",
        "\n",
        "doc.add_paragraph(\"\\nExample Python code to merge churn data with demographics:\")\n",
        "doc.add_paragraph(\"`merged_df = pd.merge(demographics_df, churn_df, on='CustomerID')`\")\n",
        "doc.add_paragraph(\n",
        "    \"This merged data is essential for:\"\n",
        ")\n",
        "doc.add_paragraph(\"- Exploratory Data Analysis (EDA)\", style='List Bullet')\n",
        "doc.add_paragraph(\"- Feature engineering\", style='List Bullet')\n",
        "doc.add_paragraph(\"- Predictive modeling\", style='List Bullet')\n",
        "\n",
        "\n",
        "doc.add_heading(\"4. Visual Insights\", level=1)\n",
        "doc.add_paragraph(\n",
        "    \"To gain deeper insights into the factors influencing churn, we have generated several visualizations:\"\n",
        ")\n",
        "\n",
        "doc.add_paragraph(\"\\n4.1 Churn Status by Gender\")\n",
        "doc.add_paragraph(\n",
        "    \"This stacked bar chart illustrates the breakdown of churned and retained customers across different genders. \"\n",
        "    \"It helps in identifying if gender plays a significant role in customer churn.\"\n",
        ")\n",
        "doc.add_picture(gender_churn_path, width=Inches(6))\n",
        "last_paragraph = doc.paragraphs[-1]\n",
        "last_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "doc.add_paragraph(\"\\n4.2 Churn Status by Income Level\")\n",
        "doc.add_paragraph(\n",
        "    \"This stacked bar chart visualizes the churn status categorized by income levels. \"\n",
        "    \"It provides insights into whether specific income groups are more prone to churn.\"\n",
        ")\n",
        "doc.add_picture(income_churn_path, width=Inches(6))\n",
        "last_paragraph = doc.paragraphs[-1]\n",
        "last_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "# --- Conclusion (Simple) ---\n",
        "doc.add_paragraph(\"\\n\")\n",
        "doc.add_heading(\"Conclusion\", level=1)\n",
        "doc.add_paragraph(\n",
        "    \"This report provides an overview of the customer churn status and highlights the importance of \"\n",
        "    \"integrating this data with other customer datasets for a comprehensive analysis. The included \"\n",
        "    \"visualizations offer initial insights into churn patterns related to gender and income level. \"\n",
        "    \"Further exploratory data analysis and predictive modeling will leverage these insights to \"\n",
        "    \"understand the root causes of churn and develop strategies for customer retention.\"\n",
        ")\n",
        "\n",
        "# Save the document\n",
        "report_filename = \"Churn_Status_Report_with_Visualizations.docx\"\n",
        "doc.save(report_filename)\n",
        "\n",
        "print(f\"The '{report_filename}' file has been created with a report-like structure including visualizations.\")\n",
        "\n",
        "# Clean up generated image files\n",
        "os.remove(churn_dist_path)\n",
        "os.remove(gender_churn_path)\n",
        "os.remove(income_churn_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "DTCUXehGFWAp",
        "outputId": "d64e7b6c-836f-4bb6-cf41-af028ac99e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "The 'Churn_Status_Report_with_Visualizations.docx' file has been created with a report-like structure including visualizations.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries if not already installed\n",
        "!pip install pandas scikit-learn matplotlib python-docx\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os # To manage image files\n",
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
        ")\n",
        "from scipy.stats import randint\n",
        "\n",
        "# --- 1. Data Loading and Merging ---\n",
        "print(\"1. Loading and merging data...\")\n",
        "\n",
        "try:\n",
        "    df_demographics = pd.read_csv(\"Customer_Churn_Data_Large.xlsx - Customer_Demographics.csv\")\n",
        "    df_transactions = pd.read_csv(\"Customer_Churn_Data_Large.xlsx - Transaction_History.csv\")\n",
        "    df_service = pd.read_csv(\"Customer_Churn_Data_Large.xlsx - Customer_Service.csv\")\n",
        "    df_online = pd.read_csv(\"Customer_Churn_Data_Large.xlsx - Online_Activity.csv\")\n",
        "    df_churn = pd.read_csv(\"Customer_Churn_Data_Large.xlsx - Churn_Status.csv\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading file: {e}. Please ensure all CSV files are in the same directory.\")\n",
        "    # Create dummy dataframes for demonstration if files are not found\n",
        "    print(\"Creating dummy data for demonstration purposes.\")\n",
        "    num_customers = 1000\n",
        "    df_demographics = pd.DataFrame({\n",
        "        'CustomerID': range(1, num_customers + 1),\n",
        "        'Age': np.random.randint(18, 70, num_customers),\n",
        "        'Gender': np.random.choice(['Male', 'Female'], num_customers),\n",
        "        'IncomeLevel': np.random.choice(['Low', 'Medium', 'High'], num_customers),\n",
        "        'Education': np.random.choice(['High School', 'Bachelors', 'Masters', 'PhD'], num_customers)\n",
        "    })\n",
        "    df_transactions = pd.DataFrame({\n",
        "        'CustomerID': np.random.randint(1, num_customers + 1, num_customers * 5),\n",
        "        'TransactionAmount': np.random.rand(num_customers * 5) * 1000,\n",
        "        'TransactionCount': np.random.randint(1, 10, num_customers * 5)\n",
        "    })\n",
        "    df_service = pd.DataFrame({\n",
        "        'CustomerID': np.random.randint(1, num_customers + 1, num_customers * 2),\n",
        "        'CallDuration': np.random.rand(num_customers * 2) * 30,\n",
        "        'ResolutionTime': np.random.rand(num_customers * 2) * 10\n",
        "    })\n",
        "    df_online = pd.DataFrame({\n",
        "        'CustomerID': np.random.randint(1, num_customers + 1, num_customers * 3),\n",
        "        'PageViews': np.random.randint(1, 50, num_customers * 3),\n",
        "        'LoginFrequency': np.random.randint(1, 15, num_customers * 3)\n",
        "    })\n",
        "    df_churn = pd.DataFrame({\n",
        "        'CustomerID': range(1, num_customers + 1),\n",
        "        'ChurnStatus': np.random.choice([0, 1], num_customers, p=[0.75, 0.25])\n",
        "    })\n",
        "\n",
        "\n",
        "# Aggregate transactional, service, and online activity data per CustomerID\n",
        "# For simplicity, we'll sum numerical columns and count occurrences for service calls\n",
        "df_transactions_agg = df_transactions.groupby('CustomerID').agg(\n",
        "    TotalTransactionAmount=('TransactionAmount', 'sum'),\n",
        "    TotalTransactionCount=('TransactionCount', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "df_service_agg = df_service.groupby('CustomerID').agg(\n",
        "    AvgCallDuration=('CallDuration', 'mean'),\n",
        "    AvgResolutionTime=('ResolutionTime', 'mean'),\n",
        "    ServiceCallCount=('CallDuration', 'count') # Count of service calls\n",
        ").reset_index()\n",
        "\n",
        "df_online_agg = df_online.groupby('CustomerID').agg(\n",
        "    TotalPageViews=('PageViews', 'sum'),\n",
        "    TotalLoginFrequency=('LoginFrequency', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "# Merge all dataframes\n",
        "df_merged = df_demographics.merge(df_churn, on='CustomerID', how='left')\n",
        "df_merged = df_merged.merge(df_transactions_agg, on='CustomerID', how='left')\n",
        "df_merged = df_merged.merge(df_service_agg, on='CustomerID', how='left')\n",
        "df_merged = df_merged.merge(df_online_agg, on='CustomerID', how='left')\n",
        "\n",
        "# Fill NaN values that resulted from left merges (customers without transaction/service/online data)\n",
        "# Assuming 0 for counts/sums and mean for averages if a customer has no records in that category\n",
        "df_merged['TotalTransactionAmount'] = df_merged['TotalTransactionAmount'].fillna(0)\n",
        "df_merged['TotalTransactionCount'] = df_merged['TotalTransactionCount'].fillna(0)\n",
        "df_merged['AvgCallDuration'] = df_merged['AvgCallDuration'].fillna(0) # Or fill with overall mean if preferred\n",
        "df_merged['AvgResolutionTime'] = df_merged['AvgResolutionTime'].fillna(0) # Or fill with overall mean if preferred\n",
        "df_merged['ServiceCallCount'] = df_merged['ServiceCallCount'].fillna(0)\n",
        "df_merged['TotalPageViews'] = df_merged['TotalPageViews'].fillna(0)\n",
        "df_merged['TotalLoginFrequency'] = df_merged['TotalLoginFrequency'].fillna(0)\n",
        "\n",
        "# Handle missing ChurnStatus if any (shouldn't be if churn_status.csv has all CustomerIDs)\n",
        "df_merged['ChurnStatus'] = df_merged['ChurnStatus'].fillna(0) # Default to 0 if churn status is missing\n",
        "\n",
        "print(\"Data merging complete. Shape:\", df_merged.shape)\n",
        "print(\"Columns:\", df_merged.columns.tolist())\n",
        "\n",
        "# --- 2. Data Preprocessing ---\n",
        "print(\"\\n2. Preprocessing data...\")\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df_merged.drop(columns=['CustomerID', 'ChurnStatus'])\n",
        "y = df_merged['ChurnStatus']\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Create preprocessing pipelines for numerical and categorical features\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "print(\"Data preprocessing setup complete.\")\n",
        "\n",
        "# --- 3. Model Building and Training (Random Forest) ---\n",
        "print(\"\\n3. Building and training the model...\")\n",
        "\n",
        "# Define the model pipeline\n",
        "model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                 ('classifier', RandomForestClassifier(random_state=42))])\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print(f\"Data split: Train set size = {X_train.shape[0]}, Test set size = {X_test.shape[0]}\")\n",
        "\n",
        "# Hyperparameter tuning using RandomizedSearchCV\n",
        "# Define a smaller parameter distribution for faster execution in this environment\n",
        "param_distributions = {\n",
        "    'classifier__n_estimators': randint(50, 200), # Number of trees\n",
        "    'classifier__max_depth': randint(5, 20),     # Max depth of trees\n",
        "    'classifier__min_samples_split': randint(2, 10),\n",
        "    'classifier__min_samples_leaf': randint(1, 5),\n",
        "    'classifier__max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "# Use StratifiedKFold for cross-validation to maintain class distribution\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    model_pipeline,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=10, # Number of parameter settings that are sampled. Reduce for faster execution.\n",
        "    cv=cv,\n",
        "    scoring='roc_auc', # Optimize for ROC-AUC due to class imbalance\n",
        "    random_state=42,\n",
        "    n_jobs=-1, # Use all available cores\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Starting Randomized Search for hyperparameter tuning...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = random_search.best_estimator_\n",
        "print(f\"Best parameters found: {random_search.best_params_}\")\n",
        "print(f\"Best ROC-AUC score on validation sets: {random_search.best_score_:.4f}\")\n",
        "\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- 4. Model Evaluation ---\n",
        "print(\"\\n4. Evaluating model performance...\")\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1] # Probability of churn\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Retained', 'Churned'])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "conf_matrix_path = 'confusion_matrix.png'\n",
        "plt.savefig(conf_matrix_path, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "roc_curve_path = 'roc_curve.png'\n",
        "plt.savefig(roc_curve_path, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"Model evaluation complete. Plots saved.\")\n",
        "\n",
        "# --- 5. Generate Report (Word Document) ---\n",
        "print(\"\\n5. Generating Word report...\")\n",
        "\n",
        "doc = Document()\n",
        "\n",
        "# Title Page\n",
        "title = doc.add_heading(\"Customer Churn Prediction Model Report\", 0)\n",
        "title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "doc.add_paragraph(\"\\n\")\n",
        "subtitle = doc.add_paragraph(\"A Machine Learning Approach for Lloyds Banking Group\")\n",
        "subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "doc.add_paragraph(\"\\n\\n\\n\")\n",
        "date_para = doc.add_paragraph(\"Date: June 1, 2025\")\n",
        "date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "doc.add_page_break()\n",
        "\n",
        "# Introduction\n",
        "doc.add_heading(\"Introduction\", level=1)\n",
        "doc.add_paragraph(\n",
        "    \"This report details the development of a robust machine learning model designed to predict \"\n",
        "    \"customer churn for Lloyds Banking Group. The objective is to provide actionable insights that \"\n",
        "    \"can inform business strategies, enabling proactive identification of at-risk customers and \"\n",
        "    \"the implementation of effective retention initiatives. This document covers the selection of \"\n",
        "    \"an appropriate algorithm, the methodology for training and validating the model, proposed \"\n",
        "    \"evaluation metrics, and recommendations for its utilization and future improvements.\"\n",
        ")\n",
        "\n",
        "# Data Loading and Merging\n",
        "doc.add_heading(\"1. Data Loading and Merging\", level=1)\n",
        "doc.add_paragraph(\n",
        "    \"The analysis began by loading five distinct datasets related to customer information: \"\n",
        "    \"`Customer_Demographics.csv`, `Transaction_History.csv`, `Customer_Service.csv`, \"\n",
        "    \"`Online_Activity.csv`, and `Churn_Status.csv`. These datasets were then merged into a \"\n",
        "    \"unified dataframe using `CustomerID` as the primary key. Transactional, service, and online \"\n",
        "    \"activity data were aggregated to a per-customer basis to create a comprehensive feature set.\"\n",
        ")\n",
        "doc.add_paragraph(\"\\nSample of merged data (first 5 rows):\")\n",
        "# Add a table for sample data\n",
        "table = doc.add_table(rows=1, cols=min(5, df_merged.shape[1])) # Limit columns for display\n",
        "hdr_cells = table.rows[0].cells\n",
        "for i, col in enumerate(df_merged.columns[:min(5, df_merged.shape[1])]):\n",
        "    hdr_cells[i].text = col\n",
        "\n",
        "for index, row in df_merged.head(5).iterrows():\n",
        "    row_cells = table.add_row().cells\n",
        "    for i, col in enumerate(df_merged.columns[:min(5, df_merged.shape[1])]):\n",
        "        row_cells[i].text = str(row[col])\n",
        "\n",
        "doc.add_paragraph(\"\\nCode snippet for data loading and merging:\")\n",
        "code_snippet_data_load = \"\"\"\n",
        "import pandas as pd\n",
        "# ... (file loading and aggregation code as above) ...\n",
        "df_merged = df_demographics.merge(df_churn, on='CustomerID', how='left')\n",
        "df_merged = df_merged.merge(df_transactions_agg, on='CustomerID', how='left')\n",
        "df_merged = df_merged.merge(df_service_agg, on='CustomerID', how='left')\n",
        "df_merged = df_merged.merge(df_online_agg, on='CustomerID', how='left')\n",
        "# ... (fillna for merged columns) ...\n",
        "\"\"\"\n",
        "# Removed style='Code'\n",
        "doc.add_paragraph(code_snippet_data_load)\n",
        "\n",
        "\n",
        "# Data Preprocessing\n",
        "doc.add_heading(\"2. Data Preprocessing\", level=1)\n",
        "doc.add_paragraph(\n",
        "    \"Before training the model, the merged dataset underwent essential preprocessing steps. \"\n",
        "    \"This involved separating features (X) from the target variable (`ChurnStatus`, y). \"\n",
        "    \"Categorical features (`Gender`, `IncomeLevel`, `Education`) were transformed using One-Hot Encoding, \"\n",
        "    \"while numerical features were scaled using StandardScaler to ensure uniform contribution to the model. \"\n",
        "    \"Missing values in aggregated columns were handled by filling with zeros, assuming no activity if no records were present.\"\n",
        ")\n",
        "doc.add_paragraph(\"\\nCode snippet for data preprocessing setup:\")\n",
        "code_snippet_preprocessing = \"\"\"\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "X = df_merged.drop(columns=['CustomerID', 'ChurnStatus'])\n",
        "y = df_merged['ChurnStatus']\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\"\"\"\n",
        "# Removed style='Code'\n",
        "doc.add_paragraph(code_snippet_preprocessing)\n",
        "\n",
        "\n",
        "# Machine Learning Algorithm Selection\n",
        "doc.add_heading(\"3. Machine Learning Algorithm Selection\", level=1)\n",
        "doc.add_paragraph(\n",
        "    \"The churn prediction problem is a binary classification task. Considering the need for a \"\n",
        "    \"balance between high predictive accuracy and interpretability, the **Random Forest** algorithm \"\n",
        "    \"was selected. Random Forests are ensemble methods known for their strong performance, \"\n",
        "    \"robustness to overfitting, and ability to provide feature importance, which is crucial for \"\n",
        "    \"understanding the drivers of churn. While Gradient Boosting Machines offer higher accuracy, \"\n",
        "    \"Random Forest provides a better balance with interpretability for business context.\"\n",
        ")\n",
        "\n",
        "# Model Building and Training\n",
        "doc.add_heading(\"4. Model Building and Training\", level=1)\n",
        "doc.add_paragraph(\n",
        "    \"The preprocessed data was split into training (80%) and test (20%) sets, with stratification \"\n",
        "    \"to maintain the original churn class distribution. A Random Forest Classifier was integrated \"\n",
        "    \"into a pipeline with the preprocessor. Hyperparameter tuning was performed using \"\n",
        "    \"`RandomizedSearchCV` with 5-fold stratified cross-validation, optimizing for ROC-AUC score \"\n",
        "    \"due to the potential class imbalance in churn data. This ensures the model generalizes well \"\n",
        "    \"to unseen data and provides reliable performance estimates.\"\n",
        ")\n",
        "doc.add_paragraph(f\"Best parameters found: {random_search.best_params_}\")\n",
        "doc.add_paragraph(f\"Best ROC-AUC score on validation sets: {random_search.best_score_:.4f}\")\n",
        "\n",
        "doc.add_paragraph(\"\\nCode snippet for model training and tuning:\")\n",
        "code_snippet_model_train = \"\"\"\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from scipy.stats import randint\n",
        "\n",
        "# ... (preprocessor definition) ...\n",
        "\n",
        "model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                 ('classifier', RandomForestClassifier(random_state=42))])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "param_distributions = {\n",
        "    'classifier__n_estimators': randint(50, 200),\n",
        "    'classifier__max_depth': randint(5, 20),\n",
        "    'classifier__min_samples_split': randint(2, 10),\n",
        "    'classifier__min_samples_leaf': randint(1, 5),\n",
        "    'classifier__max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    model_pipeline,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=10,\n",
        "    cv=cv,\n",
        "    scoring='roc_auc',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "random_search.fit(X_train, y_train)\n",
        "best_model = random_search.best_estimator_\n",
        "\"\"\"\n",
        "# Removed style='Code'\n",
        "doc.add_paragraph(code_snippet_model_train)\n",
        "\n",
        "\n",
        "# Model Performance Evaluation\n",
        "doc.add_heading(\"5. Model Performance Evaluation\", level=1)\n",
        "doc.add_paragraph(\n",
        "    \"The model's performance was rigorously evaluated on the held-out test set using a suite of metrics \"\n",
        "    \"appropriate for imbalanced classification problems. The results are as follows:\"\n",
        ")\n",
        "doc.add_paragraph(f\"Accuracy: {accuracy:.4f}\")\n",
        "doc.add_paragraph(f\"Precision (Churn): {precision:.4f}\")\n",
        "doc.add_paragraph(f\"Recall (Churn): {recall:.4f}\")\n",
        "doc.add_paragraph(f\"F1-Score (Churn): {f1:.4f}\")\n",
        "doc.add_paragraph(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "doc.add_paragraph(\"\\n**Confusion Matrix:**\")\n",
        "doc.add_paragraph(\n",
        "    \"The confusion matrix provides a detailed breakdown of correct and incorrect predictions:\"\n",
        ")\n",
        "doc.add_picture(conf_matrix_path, width=Inches(5))\n",
        "last_paragraph = doc.paragraphs[-1]\n",
        "last_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "doc.add_paragraph(f\"True Positives (TP): {conf_matrix[1, 1]} (Correctly predicted churn)\")\n",
        "doc.add_paragraph(f\"True Negatives (TN): {conf_matrix[0, 0]} (Correctly predicted retained)\")\n",
        "doc.add_paragraph(f\"False Positives (FP): {conf_matrix[0, 1]} (Incorrectly predicted churn - Type I error)\")\n",
        "doc.add_paragraph(f\"False Negatives (FN): {conf_matrix[1, 0]} (Incorrectly predicted retained - Type II error)\")\n",
        "\n",
        "\n",
        "doc.add_paragraph(\"\\n**Receiver Operating Characteristic (ROC) Curve:**\")\n",
        "doc.add_paragraph(\n",
        "    \"The ROC curve illustrates the trade-off between the True Positive Rate and False Positive Rate \"\n",
        "    \"at various classification thresholds. The Area Under the Curve (AUC) indicates the model's \"\n",
        "    \"overall ability to distinguish between churned and retained customers. An AUC of \"\n",
        "    f\"{roc_auc:.4f} suggests a good discriminative power.\"\n",
        ")\n",
        "doc.add_picture(roc_curve_path, width=Inches(5))\n",
        "last_paragraph = doc.paragraphs[-1]\n",
        "last_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "doc.add_paragraph(\"\\nCode snippet for model evaluation and plotting:\")\n",
        "code_snippet_evaluation = \"\"\"\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# ... (other metrics calculation) ...\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Retained', 'Churned'])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.savefig('confusion_matrix.png', bbox_inches='tight')\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.savefig('roc_curve.png', bbox_inches='tight')\n",
        "\"\"\"\n",
        "# Removed style='Code'\n",
        "doc.add_paragraph(code_snippet_evaluation)\n",
        "\n",
        "\n",
        "# Feature Importance\n",
        "try:\n",
        "    # Get feature importance from the trained Random Forest classifier\n",
        "    # This requires accessing the 'classifier' step of the pipeline\n",
        "    feature_importances = best_model.named_steps['classifier'].feature_importances_\n",
        "\n",
        "    # Get feature names after one-hot encoding\n",
        "    ohe_features = best_model.named_steps['preprocessor'].named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
        "    all_features = np.concatenate([numerical_features, ohe_features])\n",
        "\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': all_features,\n",
        "        'Importance': feature_importances\n",
        "    }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    doc.add_heading(\"6. Feature Importance\", level=1)\n",
        "    doc.add_paragraph(\n",
        "        \"Understanding which features contribute most to the churn prediction is vital for \"\n",
        "        \"developing targeted business strategies. The Random Forest model provides a measure of \"\n",
        "        \"feature importance:\"\n",
        "    )\n",
        "    # Add top N features to the report\n",
        "    top_n = 10\n",
        "    doc.add_paragraph(f\"\\nTop {top_n} Most Important Features:\")\n",
        "    for index, row in importance_df.head(top_n).iterrows():\n",
        "        doc.add_paragraph(f\"- {row['Feature']}: {row['Importance']:.4f}\", style='List Bullet')\n",
        "\n",
        "except Exception as e:\n",
        "    doc.add_paragraph(f\"\\nCould not determine feature importance: {e}. This might happen if dummy data is used or specific model structure isn't as expected.\")\n",
        "\n",
        "# Ways to Improve and Utilize the Model\n",
        "doc.add_heading(\"7. Ways to Improve and Utilize the Model\", level=1)\n",
        "doc.add_paragraph(\n",
        "    \"The churn prediction model is a valuable asset for Lloyds Banking Group, offering several \"\n",
        "    \"avenues for utilization and further improvement.\"\n",
        ")\n",
        "\n",
        "doc.add_heading(\"7.1 Utilizing the Model for Business Decision-Making\", level=2)\n",
        "doc.add_paragraph(\n",
        "    \"The model can be a powerful tool for proactive customer retention and strategic planning:\"\n",
        ")\n",
        "doc.add_paragraph(\"- **Proactive Interventions:** Identify customers with high predicted churn probability. These customers can be targeted with personalized offers, loyalty programs, or direct customer service outreach to address their concerns before they churn.\", style='List Bullet')\n",
        "doc.add_paragraph(\"- **Resource Optimization:** Allocate marketing and customer service resources more efficiently by focusing retention efforts on the most 'at-risk' and potentially valuable customers.\", style='List Bullet')\n",
        "doc.add_paragraph(\"- **Product & Service Enhancement:** Analyze the most influential features contributing to churn (from feature importance) to pinpoint specific pain points in existing products, services, or customer experiences. This can guide product development and service improvements.\", style='List Bullet')\n",
        "doc.add_paragraph(\"- **Customer Segmentation:** Use the churn probability scores as a new dimension for customer segmentation, allowing for more granular and effective communication strategies for different customer groups.\", style='List Bullet')\n",
        "\n",
        "doc.add_heading(\"7.2 Potential Improvements and Adjustments\", level=2)\n",
        "doc.add_paragraph(\n",
        "    \"To further enhance the model's accuracy, robustness, and applicability, consider the following:\"\n",
        ")\n",
        "doc.add_paragraph(\"- **Advanced Feature Engineering:** Explore creating more complex features, such as customer lifetime value (CLTV), recency, frequency, monetary (RFM) analysis from transaction data, or sentiment analysis from customer service interactions.\", style='List Bullet')\n",
        "doc.add_paragraph(\"- **Handling Class Imbalance:** Implement more sophisticated techniques like SMOTE (Synthetic Minority Over-sampling Technique) or ADASYN to create synthetic samples of the minority class (churners), providing a more balanced training set.\", style='List Bullet')\n",
        "doc.add_paragraph(\"- **Alternative Algorithms & Ensembling:** Experiment with other powerful algorithms like Gradient Boosting Machines (XGBoost, LightGBM) for potentially higher accuracy. Also, consider ensemble methods (e.g., stacking or blending) that combine predictions from multiple models.\", style='List Bullet')\n",
        "doc.add_paragraph(\"- **Threshold Optimization:** The default classification threshold of 0.5 might not be optimal for business. Adjust the probability threshold based on the relative costs of False Positives vs. False Negatives to maximize business value (e.g., prioritize Recall if missing churners is more costly).\", style='List Bullet')\n",
        "doc.add_paragraph(\"- **Model Monitoring and Retraining:** Implement a robust MLOps pipeline for continuous monitoring of model performance (e.g., drift detection) and automated retraining with fresh data to ensure its long-term effectiveness and adapt to changing customer behaviors.\", style='List Bullet')\n",
        "doc.add_paragraph(\"- **Explainable AI (XAI):** While Random Forests provide global feature importance, integrating local XAI techniques (e.g., SHAP values, LIME) can offer explanations for individual customer churn predictions, increasing trust and enabling more precise interventions.\", style='List Bullet')\n",
        "\n",
        "# Conclusion\n",
        "doc.add_heading(\"Conclusion\", level=1)\n",
        "doc.add_paragraph(\n",
        "    \"This report has detailed the end-to-end process of developing a machine learning model for \"\n",
        "    \"customer churn prediction for Lloyds Banking Group. By selecting a Random Forest classifier, \"\n",
        "    \"implementing rigorous preprocessing and training with hyperparameter tuning, and evaluating \"\n",
        "    \"performance with appropriate metrics, a robust predictive tool has been established. The \"\n",
        "    \"insights and recommendations provided aim to empower the business to proactively identify \"\n",
        "    \"and retain valuable customers, ultimately contributing to sustained growth and profitability.\"\n",
        ")\n",
        "\n",
        "# Save the document\n",
        "report_filename = \"Customer_Churn_Prediction_Report.docx\"\n",
        "doc.save(report_filename)\n",
        "\n",
        "print(f\"\\nReport '{report_filename}' created successfully.\")\n",
        "\n",
        "# Clean up generated image files\n",
        "if os.path.exists(conf_matrix_path):\n",
        "    os.remove(conf_matrix_path)\n",
        "if os.path.exists(roc_curve_path):\n",
        "    os.remove(roc_curve_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0eqlKbqbI4Ha",
        "outputId": "4514e864-17cb-4fb2-d1ef-767d89118903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "1. Loading and merging data...\n",
            "Error loading file: [Errno 2] No such file or directory: 'Customer_Churn_Data_Large.xlsx - Customer_Demographics.csv'. Please ensure all CSV files are in the same directory.\n",
            "Creating dummy data for demonstration purposes.\n",
            "Data merging complete. Shape: (1000, 13)\n",
            "Columns: ['CustomerID', 'Age', 'Gender', 'IncomeLevel', 'Education', 'ChurnStatus', 'TotalTransactionAmount', 'TotalTransactionCount', 'AvgCallDuration', 'AvgResolutionTime', 'ServiceCallCount', 'TotalPageViews', 'TotalLoginFrequency']\n",
            "\n",
            "2. Preprocessing data...\n",
            "Data preprocessing setup complete.\n",
            "\n",
            "3. Building and training the model...\n",
            "Data split: Train set size = 800, Test set size = 200\n",
            "Starting Randomized Search for hyperparameter tuning...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best parameters found: {'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 107}\n",
            "Best ROC-AUC score on validation sets: 0.5059\n",
            "Model training complete.\n",
            "\n",
            "4. Evaluating model performance...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7650\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1-Score: 0.0000\n",
            "ROC-AUC: 0.4980\n",
            "\n",
            "Confusion Matrix:\n",
            " [[153   0]\n",
            " [ 47   0]]\n",
            "Model evaluation complete. Plots saved.\n",
            "\n",
            "5. Generating Word report...\n",
            "\n",
            "Report 'Customer_Churn_Prediction_Report.docx' created successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}